{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3ac03e3e-9d94-4b27-a1ac-400c7efe41e1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "UGkT44rJdZpm"
      },
      "source": [
        "**Introduction to Python for Data Science and Data Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9ad490b9-983f-4239-be47-bba249b16e53",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HpQMKY-vdZpq"
      },
      "source": [
        "Spark Components\n",
        "\n",
        "  ![](https://jmp.sh/s/YzccOZReG7jdljk57jox)\n",
        "\n",
        "  Spark Core (RDD API)-> Dataframe API -> Spark SQL/ Spark R API/ MLlib/ Structured Streaming API\n",
        "\n",
        "# Spark Runtime Architecture\n",
        "Driver / Cluster Manager / Workers / Executors\n",
        "###  Driver    \n",
        "- Responsible for planning and co-ordinating execution.\n",
        "- Creates the SparkSession, the entry point to all spark applications.\n",
        "- analyzes spark application and constructs DAG\n",
        "- Schedules and distributes tasks to executors for execution\n",
        "- monitors the progress of tasks and handles failures\n",
        "- returns results to the client\n",
        "###  Cluster Manager/Master\n",
        "- Manages cluster resources and allocates them to driver\n",
        "###  Workers\n",
        " - Nodes in the cluster that host ececutors.\n",
        "###  Executors\n",
        "- Processes on worker nodes that execute tasks assigned by the driver.\n",
        "- Run on worker nodes in a spark cluster and host Tasks.\n",
        "- Store intermediate and final resluts in memory or on disk.\n",
        "- Interact with the driver for task co-ordination and data transfer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "21ad6b8d-703d-4c1b-9858-bb7e0c580531",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CPx7C7yVdZpq"
      },
      "source": [
        "# The Spark DAG\n",
        " - Spark jobs are broken down in stages i.e group of tasks that can be run in parallel.\n",
        " - Computations flow in one direction through the stages\n",
        " - Stages never loop back, ensuring the job terminates\n",
        " - Stages are organized into a dependency graph for execution flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "802397ce-230f-4a4d-87aa-f96f52290d30",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "u8PK9CqrdZpq"
      },
      "source": [
        "The Spark UI\n",
        " Visualising Spark applications\n",
        "\n",
        " Spark provides web user interfaces for monitoring and management including\n",
        "\n",
        "###  Application UI\n",
        "  - Per application SparkSession\n",
        "  - Track Application progress and task execution\n",
        "  - DAG visualization and stage details\n",
        "  - Resource usage and performance metrics.\n",
        "\n",
        "### Master UI\n",
        "  - Per cluster\n",
        "  - Worker node status and health and cluster-wide resource allocation\n",
        "  - Shows all running applications and available resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a7b16866-dcde-47dc-9b70-54f4c00e8f7e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iL2uSAgDdZpq"
      },
      "source": [
        "# Spark Clusters in databricks\n",
        "\n",
        "- **All purpose clusters** - interactive clusters that support notebooks , jobs, dashboards with auto termination\n",
        "- **Job Cluster** - Clusters that stat when a job runs and terminate automatically upon completion, optimized for non interactive workloads.\n",
        "- **SQL Warehouses** - Optimized clusters for SQL query performance with instant startup and auto-scaling to balance cost and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0d6f2c76-16bd-43a1-a6a4-54a6d7eb4c2d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fp8E6G4jmWRf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"MySparkSession\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c07da373-69cb-48e0-be98-43516110cd8d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwU5syzmlhcW",
        "outputId": "cbf41b1e-dbe1-4d64-c0b2-d32999fce495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# install ngrok reverse proxy python wrapper required in google colab to check the Spark UI\n",
        "!pip install pyngrok\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d2329039-a107-4e9b-89ad-bb7312a5d393",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5FGX4gRZloXM"
      },
      "outputs": [],
      "source": [
        "#required in google colab to check the Spark UI\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyngrok import ngrok\n",
        "from pyngrok import conf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dc017c59-b5c9-4709-aaa5-aea7613260ff",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2yDSYe9YGn5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a464cd1-d615-435f-8b6f-244e4d265085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "#required in google colab to check the Spark UI\n",
        "ngrok.set_auth_token(\"Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3cfb2fbc-ab39-4346-ba53-10eae102be1f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-Se1_tTAF09",
        "outputId": "239fd9b6-eac5-4053-aa01-d1868ea9fca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-05T13:05:39+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://a4a09b43a1c4:4040\n",
            "Spark UI URL: https://20599a4ee1f6.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "#required in google colab to check the Spark UI\n",
        "#conf.get_default().ngrok_skip_browser_warning = True\n",
        "\n",
        "spark_ui_url = spark.sparkContext.uiWebUrl\n",
        "print(spark_ui_url)\n",
        "if spark_ui_url:\n",
        "  # Extract the port from the URL\n",
        "  spark_ui_port = int(spark_ui_url.split(':')[-1])\n",
        "  ngrok_tunnel = ngrok.connect(spark_ui_port)\n",
        "  print(f\"Spark UI URL: {ngrok_tunnel.public_url}\")\n",
        "else:\n",
        "  print(\"Spark UI is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "657904a2-1c11-4bc8-b9db-9f1a211ebbcf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "zWW531_yYyI-"
      },
      "source": [
        "# DataFrames\n",
        "\n",
        "* Dataframes are distributed collection of records aall with the same pre-defined structure   \n",
        "* Built on Sparks core concepts but with structure , optimization and familar SQL like operations for data manipulation.\n",
        "* DataFrames tack their schema and provide native support for many common SQL functions and relational operators like JOINs.\n",
        "* DataFrames are evaluated as DAGs using lazy evaluation . Prepare the DAG as execute when data is requested.\n",
        "\n",
        "* Can be created from JSON,CSV, Parquet, ORC,Text or Binary Files\n",
        "* Delta Lake or other Table storage format directories.\n",
        "\n",
        "## DataFrame API Optimization\n",
        "- Adaptive Query Execution\n",
        "- In-memory Columnar Storage\n",
        "- Built in Statistics\n",
        "- Catalyst Optimizer adn Photon (DataBricks)  \n",
        "\n",
        "## DataFrame/ Query Planning\n",
        "- When a DF is evaluated, the driver creates an optimzed execution plan throught a series of transformation\n",
        " Unresolved logical plan -> Analyzed Logical Plan -> Optimized logical Plan -> Physical Plan\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "52a048fc-f283-4810-bc22-d86851bf3291",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "oP5GVVbWsb3j"
      },
      "source": [
        "#Columnar Storage\n",
        "\n",
        "\n",
        "*   Organizes data by column enabling efficient scanning and analysis\n",
        "*   Efficient for analytical workloads\n",
        "*   Implemented in dataframe internal storage and in physical file encoding formats such as Parquet and ORC.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3e43a1ae-00fa-4af7-a106-969feb607aab",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dIfc9_KZs9Mi"
      },
      "source": [
        "# DataFrameReader and DataFrameWriter\n",
        "\n",
        "df = spark.read.format(\"format\").option().load()\n",
        "\n",
        "df = spark.read.csv(\"filelocation\")\n",
        "df = spark.read.parquet(\"filelocation\")\n",
        "\n",
        "-------------\n",
        "\n",
        "df.write.format(\"format\").mode(\"mode\").save()\n",
        "\n",
        "df.write.csv(\"filelocation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Infer the schema of the dataframe Using a DDL string\n",
        "\n",
        "housing_ddl_schema = '''\n",
        "longitude DOUBLE,\n",
        "latitude DOUBLE,\n",
        "housing_median_age DOUBLE,\n",
        "total_rooms DOUBLE,\n",
        "total_bedrooms DOUBLE,\n",
        "population DOUBLE,\n",
        "households DOUBLE,\n",
        "median_income DOUBLE,\n",
        "median_house_value DOUBLE\n",
        "'''\n",
        "\n",
        "housing_ddl_df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\",\"true\")\\\n",
        ".option(\"inferSchema\",\"false\")\\\n",
        ".schema(housing_ddl_schema)\\\n",
        ".load(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "fsIa4BH6H8ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Record count\n",
        "\n",
        "housing_ddl_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtzBP_a6IiWQ",
        "outputId": "e550944b-078e-44d9-c836-1dd01bfa0c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#explicitly define the schema\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "housing_schema = StructType([\n",
        "    StructField(\"longitude\",DoubleType()),\n",
        "    StructField(\"latitude\",DoubleType()),\n",
        "    StructField(\"housing_median_age\",DoubleType()),\n",
        "    StructField(\"total_rooms\",DoubleType()),\n",
        "    StructField(\"total_bedrooms\",DoubleType()),\n",
        "    StructField(\"population\",DoubleType()),\n",
        "    StructField(\"households\",DoubleType()),\n",
        "    StructField(\"median_income\",DoubleType()),\n",
        "    StructField(\"median_house_value\",DoubleType())\n",
        "    ])\n",
        "\n",
        "housing_data_df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\",\"true\")\\\n",
        ".option(\"inferSchema\",\"false\")\\\n",
        ".schema(housing_schema)\\\n",
        ".load(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "tW5V-PQWGD2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Record count\n",
        "housing_data_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0UapUrIHXVz",
        "outputId": "0bc57d0b-bd52-4813-b1f9-632662e8af4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading data from a csv file\n",
        "#This will create a spark job to load data as it infers the Schema\n",
        "\n",
        "housing_df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\",\"true\")\\\n",
        ".option(\"inferSchema\",\"true\")\\\n",
        ".load(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jl0V9DwvEYOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the fields available\n",
        "housing_df.printSchema()\n",
        "\n",
        "#display sample data\n",
        "housing_df.show()"
      ],
      "metadata": {
        "id": "GOLDEouqFacl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46b0e55-5efa-406f-ad84-0872640d2382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
            "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
            "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
            "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
            "|  -120.65|   35.48|              19.0|     2310.0|         471.0|    1341.0|     441.0|        3.225|          166900.0|\n",
            "|  -122.84|    38.4|              15.0|     3080.0|         617.0|    1446.0|     599.0|       3.6696|          194400.0|\n",
            "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
            "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
            "|  -119.12|   35.85|              37.0|      736.0|         166.0|     564.0|     138.0|       2.4167|           58300.0|\n",
            "|  -121.93|   37.25|              36.0|     1089.0|         182.0|     535.0|     170.0|         4.69|          252600.0|\n",
            "|  -117.03|   32.97|              16.0|     3936.0|         694.0|    1935.0|     659.0|       4.5625|          231200.0|\n",
            "|  -117.97|   33.73|              27.0|     2097.0|         325.0|    1217.0|     331.0|       5.7121|          222500.0|\n",
            "|  -117.99|   33.81|              42.0|      161.0|          40.0|     157.0|      50.0|          2.2|          153100.0|\n",
            "|  -120.81|   37.53|              15.0|      570.0|         123.0|     189.0|     107.0|        1.875|          181300.0|\n",
            "|   -121.2|   38.69|              26.0|     3077.0|         607.0|    1603.0|     595.0|       2.7174|          137500.0|\n",
            "|  -118.88|   34.21|              26.0|     1590.0|         196.0|     654.0|     199.0|       6.5851|          300000.0|\n",
            "|  -122.59|   38.01|              35.0|     8814.0|        1307.0|    3450.0|    1258.0|       6.1724|          414300.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "830509c3-0826-44c8-9108-1d4ab115f317",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "K0w-rS1svMR6"
      },
      "outputs": [],
      "source": [
        "#writing data onto a file csv\n",
        "\n",
        "result.write.format(\"csv\").mode(\"overwrite\").save(\"primes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing_df.write.format(\"parquet\") \\\n",
        ".mode(\"overwrite\") \\\n",
        ".save(\"housing_parquet\")"
      ],
      "metadata": {
        "id": "-LVfJ3-SJIY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing the dataframe to a new table\n",
        "housing_df.write.saveAsTable(\"california_housing\")"
      ],
      "metadata": {
        "id": "kdudWXW2KEcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "72f6aae8-9f0b-4cdd-a3e7-4b684f1e2de5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WkNxiNMjxVaL"
      },
      "source": [
        "# Dataframe Schema\n",
        "\n",
        "- Every DF has a define schema i.e structure and data types of all columns\n",
        "- Can be inferred from data or explicitly specified\n",
        "- self describing format like parquet include schema information\n",
        "- df.printSchema() --> to print out the dataFrame schema\n",
        "- DDL schema\n",
        "    - ddl_schema = \"name STRING NOT NULL, age INT, city STRING\"\n",
        "    - df = spark.read.csv(\"Filelocation\",schema = ddl_schema)\n",
        "    - df.printSchema()\n",
        "- DataFrame Data Types (Primitive and Complex datatypes)\n",
        "    - TINYINT/SMALLINT/INT/BIGINT\n",
        "    - FLOAT/DOUBLE\n",
        "    - STRING\n",
        "    - BINARY\n",
        "    - TIMESTAMP/DATE\n",
        "    - ARRAY\n",
        "    - MAP\n",
        "    - STRUCT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "23f9bf69-24d0-4c89-8766-37f3fc128c9e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Q2xrNk2Fxseb"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "828aa2ea-30d1-48c7-a70d-a27ce3910905",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JY6-9hxE0dG4"
      },
      "source": [
        "#Transformations and Actions\n",
        "\n",
        "DF are immutable - once created their data cannot be modified\n",
        "\n",
        "\n",
        "*   **Transformations** create new DF from existing ones\n",
        "    - select/filter/withColumn/groupBy/agg\n",
        "*   **Actions** like showing or saving output trigger actual computation and produce final results.\n",
        "    - count/show/take/first/write\n",
        "    - Multiple transformations can be called, the job is only created when an action is requested - Lazy evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b1ec5393-d999-4e08-b4e9-7996dea65faa",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WRH45lat1w1W"
      },
      "source": [
        "#SparkSQL\n",
        "SQL interface for Spark DataFrames\n",
        "\n",
        "**DataFrame Registration**\n",
        "    - Temporary views: createOrReplaceTempView()\n",
        "    - Global Temp views: createGlobalTempView()\n",
        "\n",
        "**SQL Query Execution**\n",
        "    - spark.sql() for SQL statements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z7--JeRBWIRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Display all available tables\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "metadata": {
        "id": "k34lqvkMWHpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ab6d73-cb98-456d-8f71-3b58c1a494d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+-----------+\n",
            "|namespace|     tableName|isTemporary|\n",
            "+---------+--------------+-----------+\n",
            "|         |streaming_data|       true|\n",
            "+---------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display data in table\n",
        "spark.sql(\"select * from california_housing\").show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ogBqfgvTKajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2f44ca-e8be-4cb5-e1c6-3e16629eee2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
            "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
            "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
            "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
            "|  -120.65|   35.48|              19.0|     2310.0|         471.0|    1341.0|     441.0|        3.225|          166900.0|\n",
            "|  -122.84|    38.4|              15.0|     3080.0|         617.0|    1446.0|     599.0|       3.6696|          194400.0|\n",
            "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
            "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
            "|  -119.12|   35.85|              37.0|      736.0|         166.0|     564.0|     138.0|       2.4167|           58300.0|\n",
            "|  -121.93|   37.25|              36.0|     1089.0|         182.0|     535.0|     170.0|         4.69|          252600.0|\n",
            "|  -117.03|   32.97|              16.0|     3936.0|         694.0|    1935.0|     659.0|       4.5625|          231200.0|\n",
            "|  -117.97|   33.73|              27.0|     2097.0|         325.0|    1217.0|     331.0|       5.7121|          222500.0|\n",
            "|  -117.99|   33.81|              42.0|      161.0|          40.0|     157.0|      50.0|          2.2|          153100.0|\n",
            "|  -120.81|   37.53|              15.0|      570.0|         123.0|     189.0|     107.0|        1.875|          181300.0|\n",
            "|   -121.2|   38.69|              26.0|     3077.0|         607.0|    1603.0|     595.0|       2.7174|          137500.0|\n",
            "|  -118.88|   34.21|              26.0|     1590.0|         196.0|     654.0|     199.0|       6.5851|          300000.0|\n",
            "|  -122.59|   38.01|              35.0|     8814.0|        1307.0|    3450.0|    1258.0|       6.1724|          414300.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Distributed System Programming\n",
        "\n",
        "\n",
        "*   Distributed computing fundamentals\n",
        "    -   Shared nothing architecture\n",
        "        - Independence\n",
        "        - Scalability\n",
        "        - Fault tolerance\n",
        "        - Resource partitioning\n",
        "    -   Partitioning\n",
        "        - Data distribution\n",
        "            -   Data divided into mutually exclusive in-memory partitions\n",
        "            -   partitioning can be based upon input and manipulated\n",
        "            -   size and number of partitions affect parallelism        \n",
        "    -   Parallel Processing\n",
        "        -   Each partition processed independently\n",
        "        -   multiple partitions can run in parallel\n",
        "        -   one partition = one task in spark\n",
        "*   Data Movement and Shuffling\n",
        "    -   Data movement between nodes ops like groupby and Join\n",
        "        Redistribute data across partitions required for operations like groupBy it occurs when wide transformations, key based operations, data repartitioning\n",
        "*   MapReduce\n",
        "    -   MapReduce Algorithm\n",
        "        - Map/Shuffle/Reduce\n",
        "        - GroupBy: Map (extract keys)->Shuffle (by key)-> Reduce(aggregate)\n",
        "        - join   : Map (prepare keys)->Shuffle (co-locate)-> Reduce(combine)\n",
        "        - filter : Map (evaluate condition) -> No shuffle or reduce needed\n",
        "    -   Its application in RDD and DataFrame APIs"
      ],
      "metadata": {
        "id": "oaN8ruaMWWMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic ETL with dataframe API\n",
        "\n",
        "- DF operations are distributed automatically across all partitions\n",
        "- All DF transformations have equivalent SQL operations.\n",
        "- Methods return new DF rather than modifying the existing one.\n",
        "- Each operation builds logical plan until an action triggers execution(Lazy loading)\n",
        "\n",
        "```\n",
        "## Basic transformation methods and SQL equivalent\n",
        "- select()          - SELECT\n",
        "- filter(),where()  - WHERE\n",
        "- groupBy()         - GROUP BY\n",
        "- orderBy(),sort()  - ORDER BY\n",
        "- join()            - JOIN\n",
        "```\n",
        "\n",
        "## Referencing dataframe columns\n",
        "\n",
        "```\n",
        "df.select(\"column-Name\")\n",
        "df.select(df.column_name)\n",
        "df.select(df[\"column_Name\"])\n",
        "df.select(col(\"column-Name\").alias(\"CustomerName\"))\n",
        "```\n",
        "\n",
        "## Common column object methods\n",
        "\n",
        "```\n",
        "alias()\n",
        "cast()/astype()\n",
        "isNull()/isNotNull()\n",
        "contains()\n",
        "asc()/desc()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4te8hnouuifx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, when, col\n",
        "\n",
        "invalidData = housing_df.select(\n",
        "    sum(when(col(\"housing_median_age\").isNull(),1).otherwise(0)).alias(\"Null_Count\")\n",
        ")\n",
        "\n",
        "invalidData.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fbI9UZH8IA6",
        "outputId": "15b22e2c-7f27-46ff-eca1-398fcdcc1014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|Null_Count|\n",
            "+----------+\n",
            "|         0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a temporary view\n",
        "\n",
        "housing_df.selectExpr(\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\").createOrReplaceTempView(\"HousingLimitedData\")"
      ],
      "metadata": {
        "id": "w9xKaoMV9LYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List available views and table\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "metadata": {
        "id": "fFwlzOJR-aW8",
        "outputId": "337d831b-fbcc-4361-d45c-139d8301dba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+-----------+\n",
            "|namespace|     tableName|isTemporary|\n",
            "+---------+--------------+-----------+\n",
            "|         |streaming_data|       true|\n",
            "+---------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spark SQL command\n",
        "invalidRecords = spark.sql('''\n",
        "select COUNT_IF(housing_median_age IS NULL) AS NULL_AGE_COUNT\n",
        ",COUNT_IF(total_rooms IS NULL) AS NULL_TOTAL_ROOMS_COUNT\n",
        " from HousingLimitedData\n",
        " ''')\n",
        "\n",
        "invalidRecordsData = invalidRecords.show()"
      ],
      "metadata": {
        "id": "2KlfzNqe-lEW",
        "outputId": "285e4bdf-31e5-4a08-a543-f09afd48c6e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------------------+\n",
            "|NULL_AGE_COUNT|NULL_TOTAL_ROOMS_COUNT|\n",
            "+--------------+----------------------+\n",
            "|             0|                     0|\n",
            "+--------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Explain plan for SQL\n",
        "\n",
        "sql_plan = invalidRecords.explain()"
      ],
      "metadata": {
        "id": "BS4VHnHs_iDN",
        "outputId": "38c97fc3-61f9-47df-933a-e7144f9346ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[], functions=[count(if (isnotnull(housing_median_age#19)) null else isnull(housing_median_age#19)), count(if (isnotnull(total_rooms#20)) null else isnull(total_rooms#20))])\n",
            "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=184]\n",
            "      +- HashAggregate(keys=[], functions=[partial_count(if (isnotnull(housing_median_age#19)) null else isnull(housing_median_age#19)), partial_count(if (isnotnull(total_rooms#20)) null else isnull(total_rooms#20))])\n",
            "         +- FileScan csv [housing_median_age#19,total_rooms#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/sample_data/california_housing_test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<housing_median_age:double,total_rooms:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Explain plan for DF\n",
        "\n",
        "df_explainPlan = invalidData.explain()\n",
        "\n",
        "type(df_explainPlan)"
      ],
      "metadata": {
        "id": "sligA4gB_wOJ",
        "outputId": "a2917dd6-9f8a-4258-f5b7-81f8c034306d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[], functions=[sum(CASE WHEN isnull(housing_median_age#19) THEN 1 ELSE 0 END)])\n",
            "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=197]\n",
            "      +- HashAggregate(keys=[], functions=[partial_sum(CASE WHEN isnull(housing_median_age#19) THEN 1 ELSE 0 END)])\n",
            "         +- FileScan csv [housing_median_age#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/sample_data/california_housing_test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<housing_median_age:double>\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#User Defined Functions\n",
        "- UDFs allow developer to create resuable custom functions\n",
        "- has performance impact as they cannot be optimized by Catalyst optimizer and have serialization overhead.\n",
        "- Always used builtin functions if need use the Pandas functions.\n",
        "- Pandas UDFs allow you to write python functions that operate on batches of rows instead of single rows, leveraging Apache arrow for more efficient Python-JVM serialization.\n"
      ],
      "metadata": {
        "id": "OExpU42N1p0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "93f6a9f5-b667-4768-b869-ca416c207e22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_sLCt_AzdZpr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "import math\n",
        "\n",
        "# user defined function\n",
        "def primeNumbercheck(myNum):\n",
        "\tif myNum==2:\n",
        "\t\treturn True\n",
        "\telif myNum%2==0 or myNum==1:\n",
        "\t\treturn False\n",
        "\tx=int(math.sqrt(myNum))\n",
        "\tif x%2==0:\n",
        "\t\tstrtno=x+1\n",
        "\telse:\n",
        "\t\tstrtno=x\n",
        "\tfor i in range(strtno,1,-2):\n",
        "\t\tif myNum%i==0:\n",
        "\t\t\treturn False\n",
        "\treturn True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "624a9ff8-e9db-4f0c-b997-11f127f123b3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6Xt_aN37KZPV"
      },
      "outputs": [],
      "source": [
        "#register the function primecheck as a\n",
        "#User Defined Function (UDF) for use with Spark DataFrames\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(returnType=BooleanType())\n",
        "def primecheck(myNum):\n",
        "    return primeNumbercheck(myNum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c9dcce1-4b37-42d8-a7fa-33c3d3ac1f9d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQn3MSTAdZpr",
        "outputId": "23b08e14-aa5b-4c28-c5e2-be1a555cfae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "#create dataframe with sample data 1000 records\n",
        "df = spark.range(0,100)\n",
        "\n",
        "print(type(df))\n",
        "\n",
        "#Add a new column isPrime with boolean value as output\n",
        "df = df.withColumn(\"isPrime\", primecheck(df[\"id\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "84b32eda-24bd-4a19-bada-b67d0a4177fe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "VOqbY0jBdZps"
      },
      "outputs": [],
      "source": [
        "#capture the result of only prime records\n",
        "result = df.filter(df.isPrime==True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "11496d0b-dab4-46be-8066-f87c408130ef",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2TjIF71tLPhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0d53c9-bc81-4ecd-bb51-4e5b170d8f76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, isPrime: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# number of primes cache the result\n",
        "# visible on spark UI /storage/\n",
        "result.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f450b87d-4143-4df4-aeb5-8e97e7700bd1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "collapsed": true,
        "id": "iqjU-fyUNMPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0da422-c2bb-4652-ab07-e18872f03714"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=2, isPrime=True),\n",
              " Row(id=3, isPrime=True),\n",
              " Row(id=5, isPrime=True),\n",
              " Row(id=7, isPrime=True),\n",
              " Row(id=11, isPrime=True),\n",
              " Row(id=13, isPrime=True),\n",
              " Row(id=17, isPrime=True),\n",
              " Row(id=19, isPrime=True),\n",
              " Row(id=23, isPrime=True),\n",
              " Row(id=29, isPrime=True),\n",
              " Row(id=31, isPrime=True),\n",
              " Row(id=37, isPrime=True),\n",
              " Row(id=41, isPrime=True),\n",
              " Row(id=43, isPrime=True),\n",
              " Row(id=47, isPrime=True),\n",
              " Row(id=53, isPrime=True),\n",
              " Row(id=59, isPrime=True),\n",
              " Row(id=61, isPrime=True),\n",
              " Row(id=67, isPrime=True),\n",
              " Row(id=71, isPrime=True),\n",
              " Row(id=73, isPrime=True),\n",
              " Row(id=79, isPrime=True),\n",
              " Row(id=83, isPrime=True),\n",
              " Row(id=89, isPrime=True),\n",
              " Row(id=97, isPrime=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "result.collect()\n",
        "result.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a572bcc-e3db-4051-8c44-bce2f349bd83",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f76rr6kZdZps",
        "outputId": "7c4b6145-c7ed-4afd-eceb-34135ef795ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, isPrime: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Free up executor memeory by unpersisting cached objects\n",
        "result.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pandas UDF function\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "#population|households\n",
        "@pandas_udf(\"double\")\n",
        "def get_avg_house_members(populationSeries,householdsSeries):\n",
        "  return populationSeries/householdsSeries\n",
        "\n",
        "\n",
        "housing_df_enriched= housing_df.withColumn(\"avg_house_members\",get_avg_house_members(col(\"population\"),col(\"households\")))"
      ],
      "metadata": {
        "id": "5XlB38kUFbYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_df_enriched.show()"
      ],
      "metadata": {
        "id": "nqlOaAxxGyJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spark Streaming background\n",
        "\n",
        "Spark Streaming introduced in 2013 as an extension to core Spark.\n",
        "- built on the RDD API\n",
        "- Using the DStream (Discretized Streams) model.\n",
        "- Processing data in small time-based (RDD) batches.\n",
        "\n",
        "Structured Streaming was introduced in 2016\n",
        "- Built on Dataframe/dataset APIs\n",
        "- Introduced event-time processing\n",
        "- Simplified API with SQL-like operations\n",
        "- Better handling of late and out-of-order data\n",
        "\n",
        "## Microbatching\n",
        "- Microbatching processes a stream as a series of small batches\n",
        "- Data is collected into time-based chunks\n",
        "- Each chunk processed as mini-batch job typical interval of 100 ms to few seconds\n",
        "\n"
      ],
      "metadata": {
        "id": "0UQxd5gjEHUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataStreamReader and DataStreamWriter\n",
        "\n",
        "###Triggers\n",
        "- Default Trigger - Processes data as soon as the previous micro-batch completes\n",
        "- Fixed Interval - Process data at specified time intervals, useful for controlling resource usage.\n",
        "- Available Now - Process available data then stops\n",
        "\n",
        "###Output Modes for structured streaming write output results\n",
        "\n",
        "- append - default mode only adds new records to the sink.\n",
        "- update - modifies existing records and adds new ones\n",
        "- complete - writes entire result table to sink each time\n"
      ],
      "metadata": {
        "id": "t7pZeYw6HdtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.environ['SPARK_HOME'])\n",
        "\n",
        "#list the files available in the jars folder\n",
        "!ls /usr/local/lib/python3.11/dist-packages/pyspark/jars\n"
      ],
      "metadata": {
        "id": "KistbDITW6JD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OCI bucket connectivity\n",
        "\n",
        "#installation of required jars to connect to OCI\n",
        "#https://github.com/oracle/oci-hdfs-connector/releases\n",
        "\n",
        "#download the jars from below link refer the latest version from above\n",
        "!wget https://github.com/oracle/oci-hdfs-connector/releases/download/v3.4.1.0.0.3/oci-hdfs.zip\n",
        "\n",
        "#the file is downloaded into /content folder\n",
        "\n",
        "!unzip oci-hdfs.zip\n",
        "\n",
        "#move the files to jars folder\n",
        "!cp /content/third-party/lib/* /usr/local/lib/python3.11/dist-packages/pyspark/jars\n",
        "\n",
        "!cp /content/lib/* /usr/local/lib/python3.11/dist-packages/pyspark/jars\n",
        "\n",
        "#check if the two files are available in the folder\n",
        "!ls /usr/local/lib/python3.11/dist-packages/pyspark/jars |grep -i 'hdfs\\|jsr'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XFcnl5UXZ4Q",
        "outputId": "22a83e81-ff9c-4902-81aa-60fbc084832a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jackson-datatype-jsr310-2.15.2.jar\n",
            "jsr305-3.0.0.jar\n",
            "jsr305-3.0.2.jar\n",
            "oci-hdfs-full-3.4.1.0.0.3.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/local/lib/python3.11/dist-packages/pyspark/jars |grep -i 'hdfs\\|jsr'\n",
        "\n",
        "!rm /usr/local/lib/python3.11/dist-packages/pyspark/jars/jsr305-3.0.0.jar\n",
        "\n",
        "!ls /usr/local/lib/python3.11/dist-packages/pyspark/jars |grep -i 'hdfs\\|jsr'"
      ],
      "metadata": {
        "id": "48Suhi-G2kAF",
        "outputId": "c0d9ef24-c974-48ca-c0cb-2c9547ed3711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jackson-datatype-jsr310-2.15.2.jar\n",
            "jsr305-3.0.0.jar\n",
            "jsr305-3.0.2.jar\n",
            "oci-hdfs-full-3.4.1.0.0.3.jar\n",
            "jackson-datatype-jsr310-2.15.2.jar\n",
            "jsr305-3.0.2.jar\n",
            "oci-hdfs-full-3.4.1.0.0.3.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keysetup\n",
        "\n",
        "!mkdir -p /content/.oci\n",
        "!openssl genrsa -out /content/.oci/oci_api_key.pem 2048\n",
        "!chmod go-rwx /content/.oci/oci_api_key.pem\n",
        "!openssl rsa -pubout -in /content/.oci/oci_api_key.pem -out /content/.oci/oci_api_key_public.pem\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn6Q-vgAporS",
        "outputId": "18efffb8-27f1-432b-8b00-b30cb6825cda"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writing RSA key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/.oci/oci_api_key_public.pem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDP4KgMDqCTq",
        "outputId": "e98b02e2-3884-4352-ee41-fd357a2fd45b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----BEGIN PUBLIC KEY-----\n",
            "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEApTp69nC5Kjs5ktZGHUIm\n",
            "f60oHIXOr5a7cHTpHsWyTDoEmU/q3t0Wk632pwNB7JO3mp64+U2isuGxvRJjEROo\n",
            "EBgfK57nOcEe27PkHBM303Q5eLlLfPuvXwYQWJDjnxM0AOT7NkdGBIlZ82Au/fia\n",
            "zvlgKnE+b8fUn8Y38HomOj2PuPtYFD/RXWOJwDJeXcMO8t9dZjotJFC+SmhKR5eu\n",
            "j9legAxwlZ3fTUtnT04Q1eTTZSL9NwGboVOjIv/SljekF7LN4+Ky/uXEkZ5mqBV6\n",
            "kXr4Tgo2K7I0bZXPkMTsMkxNFIza3mUEplKFqY/j/rrvaFcp1YU+22CYkW5BfuYR\n",
            "EQIDAQAB\n",
            "-----END PUBLIC KEY-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "#define the schema structure\n",
        "\n",
        "#spotify_track_uri,ts,platform,ms_played,track_name,artist_name,album_name,reason_start,reason_end,shuffle,skipped\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"spotify_track_uri\",StringType()),\n",
        "    StructField(\"ts\",TimestampType()),\n",
        "    StructField(\"platform\",StringType()),\n",
        "    StructField(\"ms_played\",LongType()),\n",
        "    StructField(\"track_name\",StringType()),\n",
        "    StructField(\"artist_name\",StringType()),\n",
        "    StructField(\"album_name\",StringType()),\n",
        "    StructField(\"reason_start\",StringType()),\n",
        "    StructField(\"reason_end\",StringType()),\n",
        "    StructField(\"shuffle\",BooleanType()),\n",
        "    StructField(\"skipped\",BooleanType())\n",
        "])\n"
      ],
      "metadata": {
        "id": "-k4rYKppTXeN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "UyL__Zdhv7lm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "# Define the Spark version you are using\n",
        "spark_version = \"spark-3.5.0\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/usr/local/{spark_version}\"\n",
        "\n",
        "# Path to the OCI HDFS connector JARs\n",
        "# Use the paths you confirmed with your ls command\n",
        "oci_jar_path = \"/usr/local/lib/python3.11/dist-packages/pyspark/jars/oci-hdfs-full-3.4.1.0.0.3.jar\"\n",
        "jsr305_jar_path = \"/usr/local/lib/python3.11/dist-packages/pyspark/jars/jsr305-3.0.2.jar\"\n",
        "jackson_jsr_jar_path = \"/usr/local/lib/python3.11/dist-packages/pyspark/jars/jackson-datatype-jsr310-2.15.2.jar\"\n",
        "\n",
        "# Combine the paths into a comma-separated string for the --jars argument\n",
        "jars_string = f\"{oci_jar_path},{jsr305_jar_path},{jackson_jsr_jar_path}\"\n",
        "\n",
        "# Now, set the PYSPARK_SUBMIT_ARGS environment variable\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--jars {jars_string} pyspark-shell\"\n",
        "\n",
        "# Initialize findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "N_ECV4bdx7x6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "\n",
        "print(\"Environment variables for Spark and JARs have been set.\")\n",
        "# Build the SparkSession with OCI configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OCIStructuredStreaming\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", classpath_jars_string) \\\n",
        "    .config(\"spark.executor.extraClassPath\", classpath_jars_string) \\\n",
        "    .config(\"spark.hadoop.fs.oci.client.hostname\", userdata.get('OCI_HOST')) \\\n",
        "    .config(\"spark.hadoop.fs.oci.client.auth.tenantId\", userdata.get('OCI_TENANTID')) \\\n",
        "    .config(\"spark.hadoop.fs.oci.client.auth.userId\", userdata.get('OCI_ID')) \\\n",
        "    .config(\"spark.hadoop.fs.oci.client.auth.fingerprint\", userdata.get('OCI_FINGERPRINT')) \\\n",
        "    .config(\"spark.hadoop.fs.oci.client.auth.pemfilepath\", \"/content/.oci/oci_api_key.pem\") \\\n",
        "    .config(\"spark.hadoop.fs.oci.impl\", \"com.oracle.bmc.hdfs.OCIHDFSFileSystem\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
        "\n",
        "print(\"SparkSession created with OCI configurations.\")\n",
        "\n",
        "# # Define the input path for your OCI bucket\n",
        "# # The OCI path format is oci://<bucket-name>@<namespace>/<prefix>\n",
        "\n",
        "\n",
        "# # Use spark.readStream to read the data\n",
        "# # You need to specify the format of your files (e.g., \"json\", \"csv\", \"parquet\")\n",
        "# # You also need to provide a checkpoint location, which is crucial for\n",
        "# # Structured Streaming to maintain state and provide fault tolerance.\n",
        "# stream_df = spark.readStream \\\n",
        "#     .format(\"csv\") \\\n",
        "#     .schema(schema) \\\n",
        "#     .option(\"maxFilesPerTrigger\", 1) \\\n",
        "#     .load(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ijlTA5gfO8O",
        "outputId": "9a880d07-805e-4dc0-f667-2ea256b917f2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables for Spark and JARs have been set.\n",
            "SparkSession created with OCI configurations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"oci://ocipysparkdatasource@bm1mycqpwdo2/pysparksourcedata\"\n",
        "stream_df = spark.read.text(input_path).limit(1)\n",
        "stream_df.show()"
      ],
      "metadata": {
        "id": "7c-K55kYwYuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accessing variables from config files if needed\n",
        "!pip install python-dotenv\n",
        "\n",
        "import dotenv\n",
        "import os\n",
        "\n",
        "dotenv.load_dotenv(\"/content/config.env\")\n",
        "\n",
        "my_url = os.getenv(\"MY_OCI_BUCKET_URL\")\n",
        "\n",
        "#print(f\"The URL read from config.env is: {my_url}\")\n"
      ],
      "metadata": {
        "id": "H1CnnfsMvvDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OCI accesssetup\n",
        "!echo SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIsrlKBnU69T",
        "outputId": "02f7d2c2-e01c-4076-e810-867ea86d8d0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPARK_HOME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stream_df = spark.readStream \\\n",
        "# .format(\"csv\") \\\n",
        "# .schema(schema) \\\n",
        "# .option(\"maxFilesPerTrigger\",1) \\\n",
        "# .option(\"path\",my_url) \\\n",
        "# .load()"
      ],
      "metadata": {
        "id": "T508fMM9TR6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"isStreaming: \",stream_df.isStreaming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKUNba5I2Yh9",
        "outputId": "1e8fa933-38c2-45c9-e199-04493fdbc4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isStreaming:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(stream_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vSHm3g-v2v3T",
        "outputId": "6227d0e3-3bc8-4107-885c-acce685244ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[spotify_track_uri: string, ts: timestamp, platform: string, ms_played: bigint, track_name: string, artist_name: string, album_name: string, reason_start: string, reason_end: string, shuffle: boolean, skipped: boolean]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stream_df.show()\n",
        "\n",
        "# Correct way to display streaming data\n",
        "query = stream_df.writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"streaming_data\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n"
      ],
      "metadata": {
        "id": "q5QnD-BH3gMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the data, you can query the in-memory table\n",
        "spark.sql(\"SELECT * FROM streaming_data\").show()"
      ],
      "metadata": {
        "id": "6ysYiBRp36a1",
        "outputId": "09731c8f-67f1-4de4-9ec7-0d784e570d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------+-------+\n",
            "|   spotify_track_uri|                 ts|  platform|ms_played|          track_name|         artist_name|          album_name|reason_start|reason_end|shuffle|skipped|\n",
            "+--------------------+-------------------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------+-------+\n",
            "|   spotify_track_uri|               NULL|  platform|     NULL|          track_name|         artist_name|          album_name|reason_start|reason_end|   NULL|   NULL|\n",
            "|2J3n32GeLmMjwuAzy...|2013-07-08 02:44:34|web player|     3185| Say It, Just Say It|        The Mowgli's|Waiting For The Dawn|    autoplay|  clickrow|  false|  false|\n",
            "|1oHxIPqJyvAYHy0PV...|2013-07-08 02:45:37|web player|    61865|Drinking from the...|       Calvin Harris|           18 Months|    clickrow|  clickrow|  false|  false|\n",
            "|487OPlneJNni3NWC8...|2013-07-08 02:50:24|web player|   285386|         Born To Die|        Lana Del Rey|Born To Die - The...|    clickrow|   unknown|  false|  false|\n",
            "|5IyblF777jLZj1vGH...|2013-07-08 02:52:40|web player|   134022|    Off To The Races|        Lana Del Rey|Born To Die - The...|   trackdone|  clickrow|  false|  false|\n",
            "|0GgAAB0ZMllFhbNc3...|2013-07-08 03:17:52|web player|        0|           Half Mast|   Empire Of The Sun|  Walking On A Dream|    clickrow|   nextbtn|  false|  false|\n",
            "|50VNvhzyaSplJCKWc...|2013-07-08 03:17:52|web player|    63485|          Impossible|        James Arthur|          Impossible|    clickrow|  clickrow|  false|  false|\n",
            "|1I4EczxGBcPR3J3Ke...|2013-07-08 03:17:56|web player|        0|      We Own The Sky|                 M83|   Saturdays = Youth|     nextbtn|   nextbtn|  false|  false|\n",
            "|5arVt2Wg0zbiWwAOZ...|2013-07-08 03:17:56|web player|     1268|Higher Ground - R...|Red Hot Chili Pep...|       Mother's Milk|     nextbtn|   nextbtn|  false|  false|\n",
            "|1ixtaZc0Adil3yD1I...|2013-07-08 03:17:58|web player|        0|       Happy Up Here|            Röyksopp|       Happy Up Here|     nextbtn|   nextbtn|  false|  false|\n",
            "|2v5mpowLQNFN7NC46...|2013-07-08 03:19:11|web player|        0|             Phantom|             Justice|             Phantom|     nextbtn|  clickrow|  false|  false|\n",
            "|07hII2Rc29q4F2nTE...|2013-07-08 03:20:20|web player|    67587|Sun - Gildas Kits...|Two Door Cinema Club|The Kitsuné Speci...|    clickrow|  clickrow|  false|  false|\n",
            "|4kO7mrAPfqIrsKwUO...|2013-07-08 03:20:36|web player|    12846|       Midnight City|                 M83|Hurry Up, We're D...|    clickrow|  clickrow|  false|  false|\n",
            "|4oTIuUmpE2xdXrpon...|2013-07-08 03:21:13|web player|    36132|              Heaven|         Emeli Sandé|Our Version Of Ev...|    clickrow|  clickrow|  false|  false|\n",
            "|49h0RYK3yzWkfbVyN...|2013-07-08 03:22:51|web player|    95817|    Do I Wanna Know?|      Arctic Monkeys|    Do I Wanna Know?|    clickrow|  clickrow|  false|  false|\n",
            "|4iG2gAwKXsOcijVaV...|2013-07-08 03:22:54|web player|     1763|     Time to Pretend|                MGMT|Oracular Spectacular|    clickrow|   nextbtn|  false|  false|\n",
            "|19K3lUMJmOdeuOBTr...|2013-07-08 03:33:38|web player|    45712|        Weekend Wars|                MGMT|Oracular Spectacular|     nextbtn|   nextbtn|  false|  false|\n",
            "|5nv854ey1k43KaZ0k...|2013-07-08 03:37:30|web player|   228021|           The Youth|                MGMT|Oracular Spectacular|     nextbtn|   unknown|  false|  false|\n",
            "|3FtYbEfBqAlGO46NU...|2013-07-08 03:41:21|web player|   229589|       Electric Feel|                MGMT|Oracular Spectacular|   trackdone| trackdone|  false|  false|\n",
            "|1jJci4qxiYcOHhQR2...|2013-07-08 03:41:30|web player|     7332|                Kids|                MGMT|Oracular Spectacular|   trackdone|  clickrow|  false|  false|\n",
            "+--------------------+-------------------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple filter\n",
        "\n",
        "filtered_stream = stream_df.filter(stream_df.shuffle==False)\n"
      ],
      "metadata": {
        "id": "Ha4_oIiKCLK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtered count\n",
        "\n",
        "query = filtered_stream.writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"filteredStream\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "Zu_qpHn1DhM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select count(1) from filteredStream\").show()"
      ],
      "metadata": {
        "id": "Sha7Zo9xD7Az",
        "outputId": "b3dad9f0-43b3-4121-8f7b-b1efe57c432a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|   38141|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(filtered_stream)"
      ],
      "metadata": {
        "id": "BatFYEgeEIn7",
        "outputId": "7814c5be-365b-4780-dc0d-e99956e2a642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[spotify_track_uri: string, ts: timestamp, platform: string, ms_played: bigint, track_name: string, artist_name: string, album_name: string, reason_start: string, reason_end: string, shuffle: boolean, skipped: boolean]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "003e9425"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": "STANDARD"
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": -1,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "Mynotebook101",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}