{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac03e3e-9d94-4b27-a1ac-400c7efe41e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UGkT44rJdZpm"
   },
   "source": [
    "**Introduction to Python for Data Science and Data Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad490b9-983f-4239-be47-bba249b16e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HpQMKY-vdZpq"
   },
   "source": [
    "Spark Components\n",
    "\n",
    "  ![](https://jmp.sh/s/YzccOZReG7jdljk57jox)\n",
    "\n",
    "  Spark Core (RDD API)-> Dataframe API -> Spark SQL/ Spark R API/ MLlib/ Structured Streaming API\n",
    "\n",
    "# Spark Runtime Architecture\n",
    "Driver / Cluster Manager / Workers / Executors\n",
    "###  Driver    \n",
    "- Responsible for planning and co-ordinating execution.\n",
    "- Creates the SparkSession, the entry point to all spark applications.\n",
    "- analyzes spark application and constructs DAG\n",
    "- Schedules and distributes tasks to executors for execution\n",
    "- monitors the progress of tasks and handles failures\n",
    "- returns results to the client\n",
    "###  Cluster Manager/Master\n",
    "- Manages cluster resources and allocates them to driver\n",
    "###  Workers\n",
    " - Nodes in the cluster that host ececutors.\n",
    "###  Executors\n",
    "- Processes on worker nodes that execute tasks assigned by the driver.\n",
    "- Run on worker nodes in a spark cluster and host Tasks.\n",
    "- Store intermediate and final resluts in memory or on disk.\n",
    "- Interact with the driver for task co-ordination and data transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ad6b8d-703d-4c1b-9858-bb7e0c580531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CPx7C7yVdZpq"
   },
   "source": [
    "# The Spark DAG\n",
    " - Spark jobs are broken down in stages i.e group of tasks that can be run in parallel.\n",
    " - Computations flow in one direction through the stages\n",
    " - Stages never loop back, ensuring the job terminates\n",
    " - Stages are organized into a dependency graph for execution flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802397ce-230f-4a4d-87aa-f96f52290d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "u8PK9CqrdZpq"
   },
   "source": [
    "The Spark UI\n",
    " Visualising Spark applications\n",
    "\n",
    " Spark provides web user interfaces for monitoring and management including\n",
    "\n",
    "###  Application UI\n",
    "  - Per application SparkSession\n",
    "  - Track Application progress and task execution\n",
    "  - DAG visualization and stage details\n",
    "  - Resource usage and performance metrics.\n",
    "\n",
    "### Master UI\n",
    "  - Per cluster\n",
    "  - Worker node status and health and cluster-wide resource allocation\n",
    "  - Shows all running applications and available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7b16866-dcde-47dc-9b70-54f4c00e8f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iL2uSAgDdZpq"
   },
   "source": [
    "# Spark Clusters in databricks\n",
    "\n",
    "- **All purpose clusters** - interactive clusters that support notebooks , jobs, dashboards with auto termination\n",
    "- **Job Cluster** - Clusters that stat when a job runs and terminate automatically upon completion, optimized for non interactive workloads.\n",
    "- **SQL Warehouses** - Optimized clusters for SQL query performance with instant startup and auto-scaling to balance cost and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d6f2c76-16bd-43a1-a6a4-54a6d7eb4c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fp8E6G4jmWRf"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MySparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f6a9f5-b667-4768-b869-ca416c207e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_sLCt_AzdZpr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import math\n",
    "\n",
    "# user defined function\n",
    "def primeNumbercheck(myNum):\n",
    "\tif myNum==2:\n",
    "\t\treturn True\n",
    "\telif myNum%2==0 or myNum==1:\n",
    "\t\treturn False\n",
    "\tx=int(math.sqrt(myNum))\n",
    "\tif x%2==0:\n",
    "\t\tstrtno=x+1\n",
    "\telse:\n",
    "\t\tstrtno=x\n",
    "\tfor i in range(strtno,1,-2):\n",
    "\t\tif myNum%i==0:\n",
    "\t\t\treturn False\n",
    "\treturn True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624a9ff8-e9db-4f0c-b997-11f127f123b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6Xt_aN37KZPV"
   },
   "outputs": [],
   "source": [
    "#register the function primecheck as a\n",
    "#User Defined Function (UDF) for use with Spark DataFrames\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def primecheck(myNum):\n",
    "    return primeNumbercheck(myNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9dcce1-4b37-42d8-a7fa-33c3d3ac1f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQn3MSTAdZpr",
    "outputId": "a1d7e340-a422-4754-9d09-d169b6226bae"
   },
   "outputs": [],
   "source": [
    "#create dataframe with sample data 1000 records\n",
    "df = spark.range(0,10000000)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "#Add a new column isPrime with boolean value as output\n",
    "df = df.withColumn(\"isPrime\", primecheck(df[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b32eda-24bd-4a19-bada-b67d0a4177fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VOqbY0jBdZps"
   },
   "outputs": [],
   "source": [
    "#capture the result of only prime records\n",
    "result = df.filter(df.isPrime==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11496d0b-dab4-46be-8066-f87c408130ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2TjIF71tLPhS"
   },
   "outputs": [],
   "source": [
    "# number of primes cache the result\n",
    "# visible on spark UI /storage/\n",
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f450b87d-4143-4df4-aeb5-8e97e7700bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": true,
    "id": "iqjU-fyUNMPX"
   },
   "outputs": [],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1010f01f-265e-4f52-9482-7e679bd55a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f74lnvrSUCMe",
    "outputId": "ab8e10cf-4866-4dc7-d16a-2baab9289288"
   },
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a572bcc-e3db-4051-8c44-bce2f349bd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f76rr6kZdZps",
    "outputId": "854963e2-da52-48cd-9412-eca1fcf2a883"
   },
   "outputs": [],
   "source": [
    "# Free up executor memeory by unpersisting cached objects\n",
    "result.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c07da373-69cb-48e0-be98-43516110cd8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwU5syzmlhcW",
    "outputId": "6c24126a-918b-4241-85ad-f7f751e2f79e"
   },
   "outputs": [],
   "source": [
    "# install ngrok reverse proxy python wrapper required in google colab to check the Spark UI\n",
    "!pip install pyngrok\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2329039-a107-4e9b-89ad-bb7312a5d393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5FGX4gRZloXM"
   },
   "outputs": [],
   "source": [
    "#required in google colab to check the Spark UI\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyngrok import ngrok\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc017c59-b5c9-4709-aaa5-aea7613260ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2yDSYe9YGn5M"
   },
   "outputs": [],
   "source": [
    "#required in google colab to check the Spark UI\n",
    "ngrok.set_auth_token(\"Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cfb2fbc-ab39-4346-ba53-10eae102be1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-Se1_tTAF09",
    "outputId": "aedf6bb7-ca0d-4c62-8ea5-b5fe63ca8dec"
   },
   "outputs": [],
   "source": [
    "#required in google colab to check the Spark UI\n",
    "spark_ui_port = spark.conf.get(\"spark.ui.port\")\n",
    "ngrok_tunnel = ngrok.connect(spark_ui_port)\n",
    "print(f\"Spark UI URL: {ngrok_tunnel.public_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "657904a2-1c11-4bc8-b9db-9f1a211ebbcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zWW531_yYyI-"
   },
   "source": [
    "# DataFrames\n",
    "\n",
    "* Dataframes are distributed collection of records aall with the same pre-defined structure   \n",
    "* Built on Sparks core concepts but with structure , optimization and familar SQL like operations for data manipulation.\n",
    "* DataFrames tack their schema and provide native support for many common SQL functions and relational operators like JOINs.\n",
    "* DataFrames are evaluated as DAGs using lazy evaluation . Prepare the DAG as execute when data is requested.\n",
    "\n",
    "* Can be created from JSON,CSV, Parquet, ORC,Text or Binary Files\n",
    "* Delta Lake or other Table storage format directories.\n",
    "\n",
    "## DataFrame API Optimization\n",
    "- Adaptive Query Execution\n",
    "- In-memory Columnar Storage\n",
    "- Built in Statistics\n",
    "- Catalyst Optimizer adn Photon (DataBricks)  \n",
    "\n",
    "## DataFrame/ Query Planning\n",
    "- When a DF is evaluated, the driver creates an optimzed execution plan throught a series of transformation\n",
    " Unresolved logical plan -> Analyzed Logical Plan -> Optimized logical Plan -> Physical Plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a048fc-f283-4810-bc22-d86851bf3291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oP5GVVbWsb3j"
   },
   "source": [
    "#Columnar Storage\n",
    "\n",
    "\n",
    "*   Organizes data by column enabling efficient scanning and analysis\n",
    "*   Efficient for analytical workloads\n",
    "*   Implemented in dataframe internal storage and in physical file encoding formats such as Parquet and ORC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e43a1ae-00fa-4af7-a106-969feb607aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dIfc9_KZs9Mi"
   },
   "source": [
    "# DataFrameReader and DataFrameWriter\n",
    "\n",
    "df = spark.read.format(\"format\").option().load()\n",
    "\n",
    "df = spark.read.csv(\"filelocation\")\n",
    "df = spark.read.parquet(\"filelocation\")\n",
    "\n",
    "-------------\n",
    "\n",
    "df.write.format(\"format\").mode(\"mode\").save()\n",
    "\n",
    "df.write.csv(\"filelocation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "830509c3-0826-44c8-9108-1d4ab115f317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "K0w-rS1svMR6"
   },
   "outputs": [],
   "source": [
    "#writing data onto a file\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"prime_numbers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f6aae8-9f0b-4cdd-a3e7-4b684f1e2de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WkNxiNMjxVaL"
   },
   "source": [
    "# Dataframe Schema\n",
    "\n",
    "- Every DF has a define schema i.e structure and data types of all columns\n",
    "- Can be inferred from data or explicitly specified\n",
    "- self describing format like parquet include schema information\n",
    "- df.printSchema() --> to print out the dataFrame schema\n",
    "- DDL schema\n",
    "    - ddl_schema = \"name STRING NOT NULL, age INT, city STRING\"\n",
    "    - df = spark.read.csv(\"Filelocation\",schema = ddl_schema)\n",
    "    - df.printSchema()\n",
    "- DataFrame Data Types (Primitive and Complex datatypes)\n",
    "    - TINYINT/SMALLINT/INT/BIGINT\n",
    "    - FLOAT/DOUBLE\n",
    "    - STRING\n",
    "    - BINARY\n",
    "    - TIMESTAMP/DATE\n",
    "    - ARRAY\n",
    "    - MAP\n",
    "    - STRUCT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f9bf69-24d0-4c89-8766-37f3fc128c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2xrNk2Fxseb",
    "outputId": "ce6b6d60-e85e-43fa-eaa4-4d8f16955ade"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "828aa2ea-30d1-48c7-a70d-a27ce3910905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JY6-9hxE0dG4"
   },
   "source": [
    "#Transformations and Actions\n",
    "\n",
    "DF are immutable - once created their data cannot be modified\n",
    "\n",
    "\n",
    "*   **Transformations** create new DF from existing ones\n",
    "    - select/filter/withColumn/groupBy/agg\n",
    "*   **Actions** like showing or saving output trigger actual computation and produce final results.\n",
    "    - count/show/take/first/write\n",
    "    - Multiple transformations can be called, the job is only created when an action is requested - Lazy evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ec5393-d999-4e08-b4e9-7996dea65faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WRH45lat1w1W"
   },
   "source": [
    "#SparkSQL\n",
    "SQL interface for Spark DataFrames\n",
    "\n",
    "**DataFrame Registration**\n",
    "    - Temporary views: createOrReplaceTempView()\n",
    "    - Global Temp views: createGlobalTempView()\n",
    "\n",
    "**SQL Query Execution**\n",
    "    - spark.sql() for SQL statements\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "STANDARD"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Mynotebook101",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
