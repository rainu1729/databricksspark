{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3ac03e3e-9d94-4b27-a1ac-400c7efe41e1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "UGkT44rJdZpm"
      },
      "source": [
        "**Introduction to Python for Data Science and Data Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9ad490b9-983f-4239-be47-bba249b16e53",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HpQMKY-vdZpq"
      },
      "source": [
        "Spark Components\n",
        "\n",
        "  ![](https://jmp.sh/s/YzccOZReG7jdljk57jox)\n",
        "\n",
        "  Spark Core (RDD API)-> Dataframe API -> Spark SQL/ Spark R API/ MLlib/ Structured Streaming API\n",
        "\n",
        "# Spark Runtime Architecture\n",
        "Driver / Cluster Manager / Workers / Executors\n",
        "###  Driver    \n",
        "- Responsible for planning and co-ordinating execution.\n",
        "- Creates the SparkSession, the entry point to all spark applications.\n",
        "- analyzes spark application and constructs DAG\n",
        "- Schedules and distributes tasks to executors for execution\n",
        "- monitors the progress of tasks and handles failures\n",
        "- returns results to the client\n",
        "###  Cluster Manager/Master\n",
        "- Manages cluster resources and allocates them to driver\n",
        "###  Workers\n",
        " - Nodes in the cluster that host ececutors.\n",
        "###  Executors\n",
        "- Processes on worker nodes that execute tasks assigned by the driver.\n",
        "- Run on worker nodes in a spark cluster and host Tasks.\n",
        "- Store intermediate and final resluts in memory or on disk.\n",
        "- Interact with the driver for task co-ordination and data transfer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "21ad6b8d-703d-4c1b-9858-bb7e0c580531",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CPx7C7yVdZpq"
      },
      "source": [
        "# The Spark DAG\n",
        " - Spark jobs are broken down in stages i.e group of tasks that can be run in parallel.\n",
        " - Computations flow in one direction through the stages\n",
        " - Stages never loop back, ensuring the job terminates\n",
        " - Stages are organized into a dependency graph for execution flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "802397ce-230f-4a4d-87aa-f96f52290d30",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "u8PK9CqrdZpq"
      },
      "source": [
        "The Spark UI\n",
        " Visualising Spark applications\n",
        "\n",
        " Spark provides web user interfaces for monitoring and management including\n",
        "\n",
        "###  Application UI\n",
        "  - Per application SparkSession\n",
        "  - Track Application progress and task execution\n",
        "  - DAG visualization and stage details\n",
        "  - Resource usage and performance metrics.\n",
        "\n",
        "### Master UI\n",
        "  - Per cluster\n",
        "  - Worker node status and health and cluster-wide resource allocation\n",
        "  - Shows all running applications and available resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a7b16866-dcde-47dc-9b70-54f4c00e8f7e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iL2uSAgDdZpq"
      },
      "source": [
        "# Spark Clusters in databricks\n",
        "\n",
        "- **All purpose clusters** - interactive clusters that support notebooks , jobs, dashboards with auto termination\n",
        "- **Job Cluster** - Clusters that stat when a job runs and terminate automatically upon completion, optimized for non interactive workloads.\n",
        "- **SQL Warehouses** - Optimized clusters for SQL query performance with instant startup and auto-scaling to balance cost and performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"MySparkSession\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "fp8E6G4jmWRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "93f6a9f5-b667-4768-b869-ca416c207e22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_sLCt_AzdZpr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "import math\n",
        "\n",
        "# user defined function\n",
        "def primeNumbercheck(myNum):\n",
        "\tif myNum==2:\n",
        "\t\treturn True\n",
        "\telif myNum%2==0 or myNum==1:\n",
        "\t\treturn False\n",
        "\tx=int(math.sqrt(myNum))\n",
        "\tif x%2==0:\n",
        "\t\tstrtno=x+1\n",
        "\telse:\n",
        "\t\tstrtno=x\n",
        "\tfor i in range(strtno,1,-2):\n",
        "\t\tif myNum%i==0:\n",
        "\t\t\treturn False\n",
        "\treturn True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#register the function primecheck as a\n",
        "#User Defined Function (UDF) for use with Spark DataFrames\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(returnType=BooleanType())\n",
        "def primecheck(myNum):\n",
        "    return primeNumbercheck(myNum)"
      ],
      "metadata": {
        "id": "6Xt_aN37KZPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c9dcce1-4b37-42d8-a7fa-33c3d3ac1f9d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQn3MSTAdZpr",
        "outputId": "a1d7e340-a422-4754-9d09-d169b6226bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "#create dataframe with sample data 1000 records\n",
        "df = spark.range(0,10000000)\n",
        "\n",
        "print(type(df))\n",
        "\n",
        "#Add a new column isPrime with boolean value as output\n",
        "df = df.withColumn(\"isPrime\", primecheck(df[\"id\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "84b32eda-24bd-4a19-bada-b67d0a4177fe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "VOqbY0jBdZps"
      },
      "outputs": [],
      "source": [
        "#capture the result of only prime records\n",
        "result = df.filter(df.isPrime==True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of primes cache the result\n",
        "# visible on spark UI /storage/\n",
        "result.cache()"
      ],
      "metadata": {
        "id": "2TjIF71tLPhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.collect()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iqjU-fyUNMPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f74lnvrSUCMe",
        "outputId": "ab8e10cf-4866-4dc7-d16a-2baab9289288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "664579"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a572bcc-e3db-4051-8c44-bce2f349bd83",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f76rr6kZdZps",
        "outputId": "854963e2-da52-48cd-9412-eca1fcf2a883"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, isPrime: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "# Free up executor memeory by unpersisting cached objects\n",
        "result.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install ngrok reverse proxy python wrapper\n",
        "!pip install pyngrok\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwU5syzmlhcW",
        "outputId": "6c24126a-918b-4241-85ad-f7f751e2f79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyngrok import ngrok\n",
        "import os"
      ],
      "metadata": {
        "id": "5FGX4gRZloXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ],
      "metadata": {
        "id": "2yDSYe9YGn5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_ui_port = spark.conf.get(\"spark.ui.port\")\n",
        "ngrok_tunnel = ngrok.connect(spark_ui_port)\n",
        "print(f\"Spark UI URL: {ngrok_tunnel.public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-Se1_tTAF09",
        "outputId": "aedf6bb7-ca0d-4c62-8ea5-b5fe63ca8dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark UI URL: https://95c5-34-16-236-147.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames\n",
        "\n",
        "* Dataframes are distributed collection of records aall with the same pre-defined structure   \n",
        "* Built on Sparks core concepts but with structure , optimization and familar SQL like operations for data manipulation.\n",
        "* DataFrames tack their schema and provide native support for many common SQL functions and relational operators like JOINs.\n",
        "* DataFrames are evaluated as DAGs using lazy evaluation . Prepare the DAG as execute when data is requested.\n",
        "\n",
        "* Can be created from JSON,CSV, Parquet, ORC,Text or Binary Files\n",
        "* Delta Lake or other Table storage format directories.\n",
        "\n",
        "## DataFrame API Optimization\n",
        "- Adaptive Query Execution\n",
        "- In-memory Columnar Storage\n",
        "- Built in Statistics\n",
        "- Catalyst Optimizer adn Photon (DataBricks)  \n",
        "\n",
        "## DataFrame/ Query Planning\n",
        "- When a DF is evaluated, the driver creates an optimzed execution plan throught a series of transformation\n",
        " Unresolved logical plan -> Analyzed Logical Plan -> Optimized logical Plan -> Physical Plan\n",
        "\n"
      ],
      "metadata": {
        "id": "zWW531_yYyI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Columnar Storage\n",
        "\n",
        "\n",
        "*   Organizes data by column enabling efficient scanning and analysis\n",
        "*   Efficient for analytical workloads\n",
        "*   Implemented in dataframe internal storage and in physical file encoding formats such as Parquet and ORC.\n",
        "\n"
      ],
      "metadata": {
        "id": "oP5GVVbWsb3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrameReader and DataFrameWriter\n",
        "\n",
        "df = spark.read.format(\"format\").option().load()\n",
        "\n",
        "df = spark.read.csv(\"filelocation\")\n",
        "df = spark.read.parquet(\"filelocation\")\n",
        "\n",
        "-------------\n",
        "\n",
        "df.write.format(\"format\").mode(\"mode\").save()\n",
        "\n",
        "df.write.csv(\"filelocation\")\n"
      ],
      "metadata": {
        "id": "dIfc9_KZs9Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#writing data onto a file\n",
        "df.write.format(\"csv\").mode(\"overwrite\").save(\"prime_numbers.csv\")"
      ],
      "metadata": {
        "id": "K0w-rS1svMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe Schema\n",
        "\n",
        "- Every DF has a define schema i.e structure and data types of all columns\n",
        "- Can be inferred from data or explicitly specified\n",
        "- self describing format like parquet include schema information\n",
        "- df.printSchema() --> to print out the dataFrame schema\n",
        "- DDL schema\n",
        "    - ddl_schema = \"name STRING NOT NULL, age INT, city STRING\"\n",
        "    - df = spark.read.csv(\"Filelocation\",schema = ddl_schema)\n",
        "    - df.printSchema()\n",
        "- DataFrame Data Types (Primitive and Complex datatypes)\n",
        "    - TINYINT/SMALLINT/INT/BIGINT\n",
        "    - FLOAT/DOUBLE\n",
        "    - STRING\n",
        "    - BINARY\n",
        "    - TIMESTAMP/DATE\n",
        "    - ARRAY\n",
        "    - MAP\n",
        "    - STRUCT\n"
      ],
      "metadata": {
        "id": "WkNxiNMjxVaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2xrNk2Fxseb",
        "outputId": "ce6b6d60-e85e-43fa-eaa4-4d8f16955ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- isPrime: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformations and Actions\n",
        "\n",
        "DF are immutable - once created their data cannot be modified\n",
        "\n",
        "\n",
        "*   **Transformations** create new DF from existing ones\n",
        "    - select/filter/withColumn/groupBy/agg\n",
        "*   **Actions** like showing or saving output trigger actual computation and produce final results.\n",
        "    - count/show/take/first/write\n",
        "    - Multiple transformations can be called, the job is only created when an action is requested - Lazy evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "JY6-9hxE0dG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SparkSQL\n",
        "SQL interface for Spark DataFrames\n",
        "\n",
        "**DataFrame Registration**\n",
        "    - Temporary views: createOrReplaceTempView()\n",
        "    - Global Temp views: createGlobalTempView()\n",
        "\n",
        "**SQL Query Execution**\n",
        "    - spark.sql() for SQL statements\n"
      ],
      "metadata": {
        "id": "WRH45lat1w1W"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": "STANDARD"
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": -1,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "Mynotebook101",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}