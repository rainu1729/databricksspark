{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3ac03e3e-9d94-4b27-a1ac-400c7efe41e1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "UGkT44rJdZpm"
      },
      "source": [
        "**Introduction to Python for Data Science and Data Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9ad490b9-983f-4239-be47-bba249b16e53",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HpQMKY-vdZpq"
      },
      "source": [
        "Spark Components\n",
        "\n",
        "  ![](https://jmp.sh/s/YzccOZReG7jdljk57jox)\n",
        "\n",
        "  Spark Core (RDD API)-> Dataframe API -> Spark SQL/ Spark R API/ MLlib/ Structured Streaming API\n",
        "\n",
        "# Spark Runtime Architecture\n",
        "Driver / Cluster Manager / Workers / Executers\n",
        "###  Driver    \n",
        "- Responsible for planning and co-ordinating execution.\n",
        "- Creates the SparkSession, the entry point to all spark applications.\n",
        "- analyzes spark application and constructs DAG\n",
        "- Schedules and distributes tasks to executors for execution\n",
        "- monitors the progress of tasks and handles failures\n",
        "- returns results to the client\n",
        "###  Cluster Manager/Master\n",
        "- Manages cluster resources and allocates them to driver\n",
        "###  Workers\n",
        " - Nodes in the cluster that host ececutors.\n",
        "###  Executors\n",
        "- Processes on worker nodes that execute tasks assigned by the driver.\n",
        "- Run on worker nodes in a spark cluster and host Tasks.\n",
        "- Store intermediate and final resluts in memory or on disk.\n",
        "- Interact with the driver for task co-ordination and data transfer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "21ad6b8d-703d-4c1b-9858-bb7e0c580531",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CPx7C7yVdZpq"
      },
      "source": [
        "# The Spark DAG\n",
        " - Spark jobs are broken down in stages i.e group of tasks that can be run in parallel.\n",
        " - Computations flow in one direction through the stages\n",
        " - Stages never loop back, ensuring the job terminates\n",
        " - Stages are organized into a dependency graph for execution flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "802397ce-230f-4a4d-87aa-f96f52290d30",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "u8PK9CqrdZpq"
      },
      "source": [
        "The Spark UI\n",
        " Visualising Spark applications\n",
        "\n",
        " Spark provides web user interfaces for monitoring and management including\n",
        "\n",
        "###  Application UI\n",
        "  - Per application SparkSession\n",
        "  - Track Application progress and task execution\n",
        "  - DAG visualization and stage details\n",
        "  - Resource usage and performance metrics.\n",
        "\n",
        "### Master UI\n",
        "  - Per cluster\n",
        "  - Worker node status and health and cluster-wide resource allocation\n",
        "  - Shows all running applications and available resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a7b16866-dcde-47dc-9b70-54f4c00e8f7e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iL2uSAgDdZpq"
      },
      "source": [
        "# Spark Clusters in databricks\n",
        "\n",
        "- **All purpose clusters** - interactive clusters that support notebooks , jobs, dashboards with auto termination\n",
        "- **Job Cluster** - Clusters that stat when a job runs and terminate automatically upon completion, optimized for non interactive workloads.\n",
        "- **SQL Warehouses** - Optimized clusters for SQL query performance with instant startup and auto-scaling to balance cost and performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"MySparkSession\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "fp8E6G4jmWRf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "93f6a9f5-b667-4768-b869-ca416c207e22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_sLCt_AzdZpr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "import math\n",
        "\n",
        "# user defined function\n",
        "def primeNumbercheck(myNum):\n",
        "\tif myNum==2:\n",
        "\t\treturn True\n",
        "\telif myNum%2==0 or myNum==1:\n",
        "\t\treturn False\n",
        "\tx=int(math.sqrt(myNum))\n",
        "\tif x%2==0:\n",
        "\t\tstrtno=x+1\n",
        "\telse:\n",
        "\t\tstrtno=x\n",
        "\tfor i in range(strtno,1,-2):\n",
        "\t\tif myNum%i==0:\n",
        "\t\t\treturn False\n",
        "\treturn True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#register the function primecheck as a\n",
        "#User Defined Function (UDF) for use with Spark DataFrames\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(returnType=BooleanType())\n",
        "def primecheck(myNum):\n",
        "    return primeNumbercheck(myNum)"
      ],
      "metadata": {
        "id": "6Xt_aN37KZPV"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c9dcce1-4b37-42d8-a7fa-33c3d3ac1f9d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "AQn3MSTAdZpr",
        "outputId": "0c3020c4-e295-434d-fd4c-13de251139d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "#create dataframe with sample data 1000 records\n",
        "df = spark.range(0,100000)\n",
        "\n",
        "print(type(df))\n",
        "\n",
        "#Add a new column isPrime with boolean value as output\n",
        "df = df.withColumn(\"isPrime\", primecheck(df[\"id\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "84b32eda-24bd-4a19-bada-b67d0a4177fe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "VOqbY0jBdZps",
        "outputId": "a528d96f-0ae3-417f-ef61-ac42cbe934ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+---+-------+\n",
            "| id|isPrime|\n",
            "+---+-------+\n",
            "|  2|   true|\n",
            "|  3|   true|\n",
            "|  5|   true|\n",
            "|  7|   true|\n",
            "| 11|   true|\n",
            "| 13|   true|\n",
            "| 17|   true|\n",
            "| 19|   true|\n",
            "| 23|   true|\n",
            "| 29|   true|\n",
            "| 31|   true|\n",
            "| 37|   true|\n",
            "| 41|   true|\n",
            "| 43|   true|\n",
            "| 47|   true|\n",
            "| 53|   true|\n",
            "| 59|   true|\n",
            "| 61|   true|\n",
            "| 67|   true|\n",
            "| 71|   true|\n",
            "+---+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(type(df))\n",
        "\n",
        "df.filter(df.isPrime==True).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of primes within 1000\n",
        "df.filter(df.isPrime==True).count()"
      ],
      "metadata": {
        "id": "2TjIF71tLPhS",
        "outputId": "ef917762-98ca-44b2-f518-a418c6b4a359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9592"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "id": "iqjU-fyUNMPX",
        "outputId": "29d2c704-869c-454b-9493-a1371a686bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a572bcc-e3db-4051-8c44-bce2f349bd83",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "f76rr6kZdZps",
        "outputId": "6090d86b-a23e-419a-8343-2ab15e1967b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.27 2025-04-15\n",
            "OpenJDK Runtime Environment (build 11.0.27+6-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.27+6-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "#  spark.sql('select current_version()').show()\n",
        "!java --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install ngrok reverse proxy python wrapper\n",
        "!pip install pyngrok\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "OwU5syzmlhcW",
        "outputId": "6c24126a-918b-4241-85ad-f7f751e2f79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyngrok import ngrok\n",
        "import os"
      ],
      "metadata": {
        "id": "5FGX4gRZloXM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ],
      "metadata": {
        "id": "2yDSYe9YGn5M"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_ui_port = spark.conf.get(\"spark.ui.port\")\n",
        "ngrok_tunnel = ngrok.connect(spark_ui_port)\n",
        "print(f\"Spark UI URL: {ngrok_tunnel.public_url}\")"
      ],
      "metadata": {
        "id": "r-Se1_tTAF09",
        "outputId": "aedf6bb7-ca0d-4c62-8ea5-b5fe63ca8dec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark UI URL: https://95c5-34-16-236-147.ngrok-free.app\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": "STANDARD"
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": -1,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "Mynotebook101",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}