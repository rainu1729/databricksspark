{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3ac03e3e-9d94-4b27-a1ac-400c7efe41e1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "UGkT44rJdZpm"
      },
      "source": [
        "**Introduction to Python for Data Science and Data Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9ad490b9-983f-4239-be47-bba249b16e53",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HpQMKY-vdZpq"
      },
      "source": [
        "Spark Components\n",
        "\n",
        "  ![](https://jmp.sh/s/YzccOZReG7jdljk57jox)\n",
        "\n",
        "  Spark Core (RDD API)-> Dataframe API -> Spark SQL/ Spark R API/ MLlib/ Structured Streaming API\n",
        "\n",
        "# Spark Runtime Architecture\n",
        "Driver / Cluster Manager / Workers / Executors\n",
        "###  Driver    \n",
        "- Responsible for planning and co-ordinating execution.\n",
        "- Creates the SparkSession, the entry point to all spark applications.\n",
        "- analyzes spark application and constructs DAG\n",
        "- Schedules and distributes tasks to executors for execution\n",
        "- monitors the progress of tasks and handles failures\n",
        "- returns results to the client\n",
        "###  Cluster Manager/Master\n",
        "- Manages cluster resources and allocates them to driver\n",
        "###  Workers\n",
        " - Nodes in the cluster that host ececutors.\n",
        "###  Executors\n",
        "- Processes on worker nodes that execute tasks assigned by the driver.\n",
        "- Run on worker nodes in a spark cluster and host Tasks.\n",
        "- Store intermediate and final resluts in memory or on disk.\n",
        "- Interact with the driver for task co-ordination and data transfer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "21ad6b8d-703d-4c1b-9858-bb7e0c580531",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CPx7C7yVdZpq"
      },
      "source": [
        "# The Spark DAG\n",
        " - Spark jobs are broken down in stages i.e group of tasks that can be run in parallel.\n",
        " - Computations flow in one direction through the stages\n",
        " - Stages never loop back, ensuring the job terminates\n",
        " - Stages are organized into a dependency graph for execution flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "802397ce-230f-4a4d-87aa-f96f52290d30",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "u8PK9CqrdZpq"
      },
      "source": [
        "The Spark UI\n",
        " Visualising Spark applications\n",
        "\n",
        " Spark provides web user interfaces for monitoring and management including\n",
        "\n",
        "###  Application UI\n",
        "  - Per application SparkSession\n",
        "  - Track Application progress and task execution\n",
        "  - DAG visualization and stage details\n",
        "  - Resource usage and performance metrics.\n",
        "\n",
        "### Master UI\n",
        "  - Per cluster\n",
        "  - Worker node status and health and cluster-wide resource allocation\n",
        "  - Shows all running applications and available resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a7b16866-dcde-47dc-9b70-54f4c00e8f7e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iL2uSAgDdZpq"
      },
      "source": [
        "# Spark Clusters in databricks\n",
        "\n",
        "- **All purpose clusters** - interactive clusters that support notebooks , jobs, dashboards with auto termination\n",
        "- **Job Cluster** - Clusters that stat when a job runs and terminate automatically upon completion, optimized for non interactive workloads.\n",
        "- **SQL Warehouses** - Optimized clusters for SQL query performance with instant startup and auto-scaling to balance cost and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0d6f2c76-16bd-43a1-a6a4-54a6d7eb4c2d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fp8E6G4jmWRf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"MySparkSession\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "93f6a9f5-b667-4768-b869-ca416c207e22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_sLCt_AzdZpr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "import math\n",
        "\n",
        "# user defined function\n",
        "def primeNumbercheck(myNum):\n",
        "\tif myNum==2:\n",
        "\t\treturn True\n",
        "\telif myNum%2==0 or myNum==1:\n",
        "\t\treturn False\n",
        "\tx=int(math.sqrt(myNum))\n",
        "\tif x%2==0:\n",
        "\t\tstrtno=x+1\n",
        "\telse:\n",
        "\t\tstrtno=x\n",
        "\tfor i in range(strtno,1,-2):\n",
        "\t\tif myNum%i==0:\n",
        "\t\t\treturn False\n",
        "\treturn True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "624a9ff8-e9db-4f0c-b997-11f127f123b3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6Xt_aN37KZPV"
      },
      "outputs": [],
      "source": [
        "#register the function primecheck as a\n",
        "#User Defined Function (UDF) for use with Spark DataFrames\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(returnType=BooleanType())\n",
        "def primecheck(myNum):\n",
        "    return primeNumbercheck(myNum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c9dcce1-4b37-42d8-a7fa-33c3d3ac1f9d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQn3MSTAdZpr",
        "outputId": "23b08e14-aa5b-4c28-c5e2-be1a555cfae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "#create dataframe with sample data 1000 records\n",
        "df = spark.range(0,100)\n",
        "\n",
        "print(type(df))\n",
        "\n",
        "#Add a new column isPrime with boolean value as output\n",
        "df = df.withColumn(\"isPrime\", primecheck(df[\"id\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "84b32eda-24bd-4a19-bada-b67d0a4177fe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "VOqbY0jBdZps"
      },
      "outputs": [],
      "source": [
        "#capture the result of only prime records\n",
        "result = df.filter(df.isPrime==True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "11496d0b-dab4-46be-8066-f87c408130ef",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2TjIF71tLPhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0d53c9-bc81-4ecd-bb51-4e5b170d8f76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, isPrime: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# number of primes cache the result\n",
        "# visible on spark UI /storage/\n",
        "result.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f450b87d-4143-4df4-aeb5-8e97e7700bd1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "collapsed": true,
        "id": "iqjU-fyUNMPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0da422-c2bb-4652-ab07-e18872f03714"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=2, isPrime=True),\n",
              " Row(id=3, isPrime=True),\n",
              " Row(id=5, isPrime=True),\n",
              " Row(id=7, isPrime=True),\n",
              " Row(id=11, isPrime=True),\n",
              " Row(id=13, isPrime=True),\n",
              " Row(id=17, isPrime=True),\n",
              " Row(id=19, isPrime=True),\n",
              " Row(id=23, isPrime=True),\n",
              " Row(id=29, isPrime=True),\n",
              " Row(id=31, isPrime=True),\n",
              " Row(id=37, isPrime=True),\n",
              " Row(id=41, isPrime=True),\n",
              " Row(id=43, isPrime=True),\n",
              " Row(id=47, isPrime=True),\n",
              " Row(id=53, isPrime=True),\n",
              " Row(id=59, isPrime=True),\n",
              " Row(id=61, isPrime=True),\n",
              " Row(id=67, isPrime=True),\n",
              " Row(id=71, isPrime=True),\n",
              " Row(id=73, isPrime=True),\n",
              " Row(id=79, isPrime=True),\n",
              " Row(id=83, isPrime=True),\n",
              " Row(id=89, isPrime=True),\n",
              " Row(id=97, isPrime=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "result.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1010f01f-265e-4f52-9482-7e679bd55a9f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f74lnvrSUCMe",
        "outputId": "12f7e3df-ea68-416c-ccb1-2ee2fa07856d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "result.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a572bcc-e3db-4051-8c44-bce2f349bd83",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f76rr6kZdZps",
        "outputId": "7c4b6145-c7ed-4afd-eceb-34135ef795ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, isPrime: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Free up executor memeory by unpersisting cached objects\n",
        "result.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c07da373-69cb-48e0-be98-43516110cd8d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwU5syzmlhcW",
        "outputId": "f5327eba-8379-4f2d-eeb9-dc97cffe3fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# install ngrok reverse proxy python wrapper required in google colab to check the Spark UI\n",
        "!pip install pyngrok\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d2329039-a107-4e9b-89ad-bb7312a5d393",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5FGX4gRZloXM"
      },
      "outputs": [],
      "source": [
        "#required in google colab to check the Spark UI\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyngrok import ngrok\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dc017c59-b5c9-4709-aaa5-aea7613260ff",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2yDSYe9YGn5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f9d5f9-35dc-457f-8aa1-38f002e04db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "#required in google colab to check the Spark UI\n",
        "ngrok.set_auth_token(\"Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3cfb2fbc-ab39-4346-ba53-10eae102be1f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-Se1_tTAF09",
        "outputId": "d227be09-60b8-4ea5-c1ca-0b3d4a802130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-06-25T02:58:57+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://e615004a3e81:4040\n",
            "Spark UI URL: https://d799-34-125-255-91.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "#required in google colab to check the Spark UI\n",
        "spark_ui_url = spark.sparkContext.uiWebUrl\n",
        "print(spark_ui_url)\n",
        "if spark_ui_url:\n",
        "  # Extract the port from the URL\n",
        "  spark_ui_port = int(spark_ui_url.split(':')[-1])\n",
        "  ngrok_tunnel = ngrok.connect(spark_ui_port)\n",
        "  print(f\"Spark UI URL: {ngrok_tunnel.public_url}\")\n",
        "else:\n",
        "  print(\"Spark UI is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "657904a2-1c11-4bc8-b9db-9f1a211ebbcf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "zWW531_yYyI-"
      },
      "source": [
        "# DataFrames\n",
        "\n",
        "* Dataframes are distributed collection of records aall with the same pre-defined structure   \n",
        "* Built on Sparks core concepts but with structure , optimization and familar SQL like operations for data manipulation.\n",
        "* DataFrames tack their schema and provide native support for many common SQL functions and relational operators like JOINs.\n",
        "* DataFrames are evaluated as DAGs using lazy evaluation . Prepare the DAG as execute when data is requested.\n",
        "\n",
        "* Can be created from JSON,CSV, Parquet, ORC,Text or Binary Files\n",
        "* Delta Lake or other Table storage format directories.\n",
        "\n",
        "## DataFrame API Optimization\n",
        "- Adaptive Query Execution\n",
        "- In-memory Columnar Storage\n",
        "- Built in Statistics\n",
        "- Catalyst Optimizer adn Photon (DataBricks)  \n",
        "\n",
        "## DataFrame/ Query Planning\n",
        "- When a DF is evaluated, the driver creates an optimzed execution plan throught a series of transformation\n",
        " Unresolved logical plan -> Analyzed Logical Plan -> Optimized logical Plan -> Physical Plan\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "52a048fc-f283-4810-bc22-d86851bf3291",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "oP5GVVbWsb3j"
      },
      "source": [
        "#Columnar Storage\n",
        "\n",
        "\n",
        "*   Organizes data by column enabling efficient scanning and analysis\n",
        "*   Efficient for analytical workloads\n",
        "*   Implemented in dataframe internal storage and in physical file encoding formats such as Parquet and ORC.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3e43a1ae-00fa-4af7-a106-969feb607aab",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dIfc9_KZs9Mi"
      },
      "source": [
        "# DataFrameReader and DataFrameWriter\n",
        "\n",
        "df = spark.read.format(\"format\").option().load()\n",
        "\n",
        "df = spark.read.csv(\"filelocation\")\n",
        "df = spark.read.parquet(\"filelocation\")\n",
        "\n",
        "-------------\n",
        "\n",
        "df.write.format(\"format\").mode(\"mode\").save()\n",
        "\n",
        "df.write.csv(\"filelocation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Infer the schema of the dataframe Using a DDL string\n",
        "\n",
        "housing_ddl_schema = '''\n",
        "longitude DOUBLE,\n",
        "latitude DOUBLE,\n",
        "housing_median_age DOUBLE,\n",
        "total_rooms DOUBLE,\n",
        "total_bedrooms DOUBLE,\n",
        "population DOUBLE,\n",
        "households DOUBLE,\n",
        "median_income DOUBLE,\n",
        "median_house_value DOUBLE\n",
        "'''\n",
        "\n",
        "housing_ddl_df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\",\"true\")\\\n",
        ".option(\"inferSchema\",\"false\")\\\n",
        ".schema(housing_ddl_schema)\\\n",
        ".load(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "fsIa4BH6H8ux"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Record count\n",
        "\n",
        "housing_ddl_df.count()"
      ],
      "metadata": {
        "id": "dtzBP_a6IiWQ",
        "outputId": "e550944b-078e-44d9-c836-1dd01bfa0c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#explicitly define the schema\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "housing_schema = StructType([\n",
        "    StructField(\"longitude\",DoubleType()),\n",
        "    StructField(\"latitude\",DoubleType()),\n",
        "    StructField(\"housing_median_age\",DoubleType()),\n",
        "    StructField(\"total_rooms\",DoubleType()),\n",
        "    StructField(\"total_bedrooms\",DoubleType()),\n",
        "    StructField(\"population\",DoubleType()),\n",
        "    StructField(\"households\",DoubleType()),\n",
        "    StructField(\"median_income\",DoubleType()),\n",
        "    StructField(\"median_house_value\",DoubleType())\n",
        "    ])\n",
        "\n",
        "housing_data_df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\",\"true\")\\\n",
        ".option(\"inferSchema\",\"false\")\\\n",
        ".schema(housing_schema)\\\n",
        ".load(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "tW5V-PQWGD2b"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_df.count()"
      ],
      "metadata": {
        "id": "r0UapUrIHXVz",
        "outputId": "0bc57d0b-bd52-4813-b1f9-632662e8af4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading data from a csv file\n",
        "#This will create a spark job to load data as it infers the Schema\n",
        "\n",
        "housing_df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\",\"true\")\\\n",
        ".option(\"inferSchema\",\"true\")\\\n",
        ".load(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jl0V9DwvEYOd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the fields available\n",
        "housing_df.printSchema()\n",
        "\n",
        "#display sample data\n",
        "housing_df.show()"
      ],
      "metadata": {
        "id": "GOLDEouqFacl",
        "outputId": "2cf84124-dc70-476e-c007-264021160e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
            "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
            "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
            "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
            "|  -120.65|   35.48|              19.0|     2310.0|         471.0|    1341.0|     441.0|        3.225|          166900.0|\n",
            "|  -122.84|    38.4|              15.0|     3080.0|         617.0|    1446.0|     599.0|       3.6696|          194400.0|\n",
            "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
            "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
            "|  -119.12|   35.85|              37.0|      736.0|         166.0|     564.0|     138.0|       2.4167|           58300.0|\n",
            "|  -121.93|   37.25|              36.0|     1089.0|         182.0|     535.0|     170.0|         4.69|          252600.0|\n",
            "|  -117.03|   32.97|              16.0|     3936.0|         694.0|    1935.0|     659.0|       4.5625|          231200.0|\n",
            "|  -117.97|   33.73|              27.0|     2097.0|         325.0|    1217.0|     331.0|       5.7121|          222500.0|\n",
            "|  -117.99|   33.81|              42.0|      161.0|          40.0|     157.0|      50.0|          2.2|          153100.0|\n",
            "|  -120.81|   37.53|              15.0|      570.0|         123.0|     189.0|     107.0|        1.875|          181300.0|\n",
            "|   -121.2|   38.69|              26.0|     3077.0|         607.0|    1603.0|     595.0|       2.7174|          137500.0|\n",
            "|  -118.88|   34.21|              26.0|     1590.0|         196.0|     654.0|     199.0|       6.5851|          300000.0|\n",
            "|  -122.59|   38.01|              35.0|     8814.0|        1307.0|    3450.0|    1258.0|       6.1724|          414300.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "830509c3-0826-44c8-9108-1d4ab115f317",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "K0w-rS1svMR6"
      },
      "outputs": [],
      "source": [
        "#writing data onto a file\n",
        "result.write.format(\"csv\").mode(\"overwrite\").save(\"primes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "72f6aae8-9f0b-4cdd-a3e7-4b684f1e2de5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WkNxiNMjxVaL"
      },
      "source": [
        "# Dataframe Schema\n",
        "\n",
        "- Every DF has a define schema i.e structure and data types of all columns\n",
        "- Can be inferred from data or explicitly specified\n",
        "- self describing format like parquet include schema information\n",
        "- df.printSchema() --> to print out the dataFrame schema\n",
        "- DDL schema\n",
        "    - ddl_schema = \"name STRING NOT NULL, age INT, city STRING\"\n",
        "    - df = spark.read.csv(\"Filelocation\",schema = ddl_schema)\n",
        "    - df.printSchema()\n",
        "- DataFrame Data Types (Primitive and Complex datatypes)\n",
        "    - TINYINT/SMALLINT/INT/BIGINT\n",
        "    - FLOAT/DOUBLE\n",
        "    - STRING\n",
        "    - BINARY\n",
        "    - TIMESTAMP/DATE\n",
        "    - ARRAY\n",
        "    - MAP\n",
        "    - STRUCT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "23f9bf69-24d0-4c89-8766-37f3fc128c9e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Q2xrNk2Fxseb"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "828aa2ea-30d1-48c7-a70d-a27ce3910905",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JY6-9hxE0dG4"
      },
      "source": [
        "#Transformations and Actions\n",
        "\n",
        "DF are immutable - once created their data cannot be modified\n",
        "\n",
        "\n",
        "*   **Transformations** create new DF from existing ones\n",
        "    - select/filter/withColumn/groupBy/agg\n",
        "*   **Actions** like showing or saving output trigger actual computation and produce final results.\n",
        "    - count/show/take/first/write\n",
        "    - Multiple transformations can be called, the job is only created when an action is requested - Lazy evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b1ec5393-d999-4e08-b4e9-7996dea65faa",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WRH45lat1w1W"
      },
      "source": [
        "#SparkSQL\n",
        "SQL interface for Spark DataFrames\n",
        "\n",
        "**DataFrame Registration**\n",
        "    - Temporary views: createOrReplaceTempView()\n",
        "    - Global Temp views: createGlobalTempView()\n",
        "\n",
        "**SQL Query Execution**\n",
        "    - spark.sql() for SQL statements\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": "STANDARD"
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": -1,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "Mynotebook101",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}