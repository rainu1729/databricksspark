{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac03e3e-9d94-4b27-a1ac-400c7efe41e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Introduction to Python for Data Science and Data Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f9556c-26c4-4830-a3cb-e6a68efa8abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "class LowBalanceError(Exception):\n",
    "    \"\"\"Exception raised when an account balance is too low.\"\"\"\n",
    "    def __init__(self, current_balance, required_amount, message=\"Insufficient balance\"):\n",
    "        self.current_balance = current_balance\n",
    "        self.required_amount = required_amount\n",
    "        self.message = f\"{message}. Current: {current_balance}, Required: {required_amount}\"\n",
    "        super().__init__(self.message)\n",
    "\n",
    "def withdraw(account_balance, amount):\n",
    "    if amount <= 0:\n",
    "        raise ValueError(\"Withdrawal amount must be positive.\")\n",
    "    if account_balance < amount:\n",
    "        raise LowBalanceError(account_balance, amount)\n",
    "    if (account_balance - amount)<100:\n",
    "        raise ValueError(\"Minimum balance should be maintained\")\n",
    "    return account_balance - amount\n",
    "\n",
    "# --- Usage ---\n",
    "account_bal = 700\n",
    "\n",
    "try:\n",
    "    new_bal = withdraw(account_bal, 700)\n",
    "    print(f\"Withdrawal successful. New balance: {new_bal}\")\n",
    "except LowBalanceError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Current balance: {e.current_balance}, Attempted withdrawal: {e.required_amount}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Input Error: {e}\")\n",
    "except Exception as e: # Catch any other unexpected errors\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "print(\"\\n--- Another attempt ---\")\n",
    "try:\n",
    "    new_bal = withdraw(account_bal, -100)\n",
    "    print(f\"Withdrawal successful. New balance: {new_bal}\")\n",
    "except LowBalanceError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Input Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Successful attempt ---\")\n",
    "try:\n",
    "    new_bal = withdraw(account_bal, 200)\n",
    "    print(f\"Withdrawal successful. New balance: {new_bal}\")\n",
    "except LowBalanceError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Input Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9529553-7ee9-46e2-a6a2-7de1a1f9e16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad490b9-983f-4239-be47-bba249b16e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark Components \n",
    "\n",
    "  ![](https://jmp.sh/s/YzccOZReG7jdljk57jox)\n",
    "\n",
    "  Spark Core (RDD API)-> Dataframe API -> Spark SQL/ Spark R API/ MLlib/ Structured Streaming API\n",
    "\n",
    "# Spark Runtime Architecture \n",
    "Driver / Cluster Manager / Workers / Executers \n",
    "###  Driver    \n",
    "- Responsible for planning and co-ordinating execution.\n",
    "- Creates the SparkSession, the entry point to all spark applications.\n",
    "- analyzes spark application and constructs DAG\n",
    "- Schedules and distributes tasks to executors for execution\n",
    "- monitors the progress of tasks and handles failures\n",
    "- returns results to the client \n",
    "###  Cluster Manager/Master\n",
    "- Manages cluster resources and allocates them to driver\n",
    "###  Workers\n",
    " - Nodes in the cluster that host ececutors.\n",
    "###  Executors \n",
    "- Processes on worker nodes that execute tasks assigned by the driver.\n",
    "- Run on worker nodes in a spark cluster and host Tasks.\n",
    "- Store intermediate and final resluts in memory or on disk.\n",
    "- Interact with the driver for task co-ordination and data transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ad6b8d-703d-4c1b-9858-bb7e0c580531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# The Spark DAG \n",
    " - Spark jobs are broken down in stages i.e group of tasks that can be run in parallel.\n",
    " - Computations flow in one direction through the stages \n",
    " - Stages never loop back, ensuring the job terminates \n",
    " - Stages are organized into a dependency graph for execution flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802397ce-230f-4a4d-87aa-f96f52290d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Spark UI\n",
    " Visualising Spark applications \n",
    "\n",
    " Spark provides web user interfaces for monitoring and management including\n",
    "\n",
    "###  Application UI \n",
    "  - Per application SparkSession \n",
    "  - Track Application progress and task execution \n",
    "  - DAG visualization and stage details\n",
    "  - Resource usage and performance metrics.\n",
    "\n",
    "### Master UI \n",
    "  - Per cluster\n",
    "  - Worker node status and health and cluster-wide resource allocation \n",
    "  - Shows all running applications and available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7b16866-dcde-47dc-9b70-54f4c00e8f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Clusters in databricks \n",
    "\n",
    "- **All purpose clusters** - interactive clusters that support notebooks , jobs, dashboards with auto termination\n",
    "- **Job Cluster** - Clusters that stat when a job runs and terminate automatically upon completion, optimized for non interactive workloads.\n",
    "- **SQL Warehouses** - Optimized clusters for SQL query performance with instant startup and auto-scaling to balance cost and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f6a9f5-b667-4768-b869-ca416c207e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import math\n",
    "def primecheck(myNum):\n",
    "\tif myNum==2:\n",
    "\t\treturn True\n",
    "\telif myNum%2==0 or myNum==1:\n",
    "\t\treturn False\n",
    "\tx=int(math.sqrt(myNum))\n",
    "\tif x%2==0:\n",
    "\t\tstrtno=x+1\n",
    "\telse:\n",
    "\t\tstrtno=x\n",
    "\tfor i in range(strtno,1,-2):\n",
    "\t\tif myNum%i==0:\n",
    "\t\t\treturn False\n",
    "\treturn True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9dcce1-4b37-42d8-a7fa-33c3d3ac1f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import *\n",
    "import time \n",
    "\n",
    "df = spark.range(0,100)\n",
    "df = df.withColumn(\"isPrime\", call_udf(\"primecheck\", col(\"id\")))\n",
    "df = df.filter(col(\"isPrime\")==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b32eda-24bd-4a19-bada-b67d0a4177fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Mynotebook101",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
